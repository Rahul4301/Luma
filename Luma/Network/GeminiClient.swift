// Luma MVP - autogenerated by Cursor
import Foundation

/// Client for Gemini API calls (BYO key only in MVP).
/// 
/// Per SECURITY.md: Keys stored in Keychain, never in code or logs.
/// Per AGENTS.md: All network calls must be explicit and attributable to user request.
/// 
/// MVP: This is a stub that returns canned responses. Real network implementation would:
/// - Use URLSession to POST to https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent
/// - Set Authorization header: "Bearer \(apiKey)" where apiKey comes from Keychain via apiKeyProvider
/// - Parse JSON response into LLMResponse format
/// - Handle streaming responses (SRS NFR-8) if supported
final class GeminiClient {
    
    private let apiKeyProvider: () -> String?
    
    /// Initializes client with a closure that provides the API key.
    /// Key should be retrieved from Keychain (via KeychainManager) when needed.
    /// Per SECURITY.md: Never store keys in code, UserDefaults, or logs.
    init(apiKeyProvider: @escaping () -> String?) {
        self.apiKeyProvider = apiKeyProvider
    }
    
    /// Generates a response from Gemini API.
    /// 
    /// MVP stub: Returns a deterministic canned LLMResponse after 100ms delay.
    /// No network calls are made in this implementation.
    /// 
    /// Real implementation would:
    /// 1. Check apiKeyProvider() returns non-nil key (fail closed if nil)
    /// 2. Build URLRequest with Authorization header: "Bearer \(key)"
    /// 3. POST prompt + optional context to Gemini API endpoint
    /// 4. Parse JSON response into LLMResponse format
    /// 5. Handle errors (network, API, parsing) per SECURITY.md fail-closed policy
    /// 
    /// Per AGENTS.md: Context is only sent when user explicitly requests (selected text/excerpt).
    /// - Parameters:
    ///   - prompt: User's prompt/query
    ///   - context: Optional context (selected text or excerpt) - only when user requests
    ///   - completion: Callback with Result containing LLMResponse JSON Data or Error
    func generate(prompt: String, context: String?, completion: @escaping (Result<Data, Error>) -> Void) {
        // Enforce: fail closed if no API key
        guard apiKeyProvider() != nil else {
            DispatchQueue.main.async {
                completion(.failure(NoAPIKeyError()))
            }
            return
        }
        
        // MVP stub: Return canned response after 100ms delay
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
            let cannedResponse = LLMResponse(
                text: "This is a canned MVP response. Real implementation would call Gemini API.",
                action: nil
            )
            
            do {
                let encoder = JSONEncoder()
                let jsonData = try encoder.encode(cannedResponse)
                completion(.success(jsonData))
            } catch {
                completion(.failure(error))
            }
        }
    }
}

/// Error returned when API key is not available.
struct NoAPIKeyError: LocalizedError {
    var errorDescription: String? {
        "No API key available. Please configure your Gemini API key in Settings."
    }
}
